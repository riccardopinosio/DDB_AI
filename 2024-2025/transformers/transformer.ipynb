{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch # pytorch\n",
    "\n",
    "torch.cuda.is_available() # i am running torch on cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the imbd dataset for sentiment classification. It already has labels (1 for positive, 0 for negative sentiment).\n",
    "- See it here: https://huggingface.co/datasets/stanfordnlp/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(2000))  # Limit for faster execution\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\" # model name from huggingface repo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # load the tokenizer model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # load the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above warning is normal. What is happening here is that we are taking the distilbert model, which is just a (smaller) bert model whose final layer is the last encoding layer (no linear layer). Hence, this model just computes vector embeddings for the tokens in a sequence. Then, with AutoModelForSequenceClassification we are putting a linear head on top to take the tokens, mean pool them, and classify the resulting vector into \"positive\" or \"negative\".\n",
    "\n",
    "We can visualize this model (using an [onnx](https://onnx.ai/) version) using [netron](https://github.com/lutzroeder/netron): see [here](https://netron.app/?url=https://huggingface.co/onnxport/distilbert-base-uncased-onnx/blob/main/model.onnx).\n",
    "\n",
    "Note how the last node (the output) is named \"last_hidden_layer\". If you click on it you see that the dimension of the layer is: (batch_size, sequence_length, 764).\n",
    "\n",
    "- Batch size is the size of the batch that passed through the model\n",
    "- Sequence length is the max number of tokens in the batch\n",
    "- 764 is the size of the token embeddings\n",
    "\n",
    "We can use onnx to export the model we just initialized and if we visualize it in netron we see that a linear head has been added on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is a sample input for ONNX export.\", return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                                          # Model to export\n",
    "    (inputs[\"input_ids\"].cuda(), inputs[\"attention_mask\"].cuda()), # Input example (tuple)\n",
    "    \"distilbert_model_binary_classification.onnx\",                         # Path to save ONNX model\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],     # Names for the inputs\n",
    "    output_names=[\"output\"],                         # Names for the outputs\n",
    "    dynamic_axes={                                   # Dynamic axes for variable sequence lengths\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=14                                # ONNX opset version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the transformer model using the transformers library Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "test_data = test_data.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids is the list of token ids in the input text, and attention_mask is the attention mask vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpinosio/miniconda3/envs/teaching/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the transformer. This will take a bit (note: if you are running on gpu, you can use `nvidia-smi` to monitor gpu utilization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.311137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.465389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.46538928151130676,\n",
       " 'eval_runtime': 9.4743,\n",
       " 'eval_samples_per_second': 52.775,\n",
       " 'eval_steps_per_second': 6.65,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above is training the transformer model and updating all the model parameters with backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: distilbert.embeddings.word_embeddings.weight | Shape: torch.Size([30522, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.position_embeddings.weight | Shape: torch.Size([512, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.LayerNorm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.LayerNorm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: pre_classifier.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: pre_classifier.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: classifier.weight | Shape: torch.Size([2, 768]) | Requires grad: True\n",
      "Parameter name: classifier.bias | Shape: torch.Size([2]) | Requires grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name} | Shape: {param.shape} | Requires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer(\"this is really bad\", return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculations for inference\n",
    "    outputs = model(input_ids=tok[\"input_ids\"].cuda(), attention_mask=tok[\"attention_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2898, -2.6190]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
