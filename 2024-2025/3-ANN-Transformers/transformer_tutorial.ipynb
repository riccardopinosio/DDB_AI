{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpinosio/miniconda3/envs/teaching/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch # pytorch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the imbd dataset for sentiment classification. It already has labels (1 for positive, 0 for negative sentiment).\n",
    "- See it here: https://huggingface.co/datasets/stanfordnlp/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(2000))  # Limit for faster execution\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\" # model name from huggingface repo\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # load the tokenizer model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # load the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above warning is normal. What is happening here is that we are taking the distilbert model, which is just a (smaller) bert model whose final layer is the last encoding layer (no linear layer). Hence, this model just computes vector embeddings for the tokens in a sequence. Then, with AutoModelForSequenceClassification we are putting a linear head on top to take the tokens, mean pool them, and classify the resulting vector into \"positive\" or \"negative\".\n",
    "\n",
    "We can visualize this model (using an [onnx](https://onnx.ai/) version) using [netron](https://github.com/lutzroeder/netron): see [here](https://netron.app/?url=https://huggingface.co/onnxport/distilbert-base-uncased-onnx/blob/main/model.onnx).\n",
    "\n",
    "Note how the last node (the output) is named \"last_hidden_layer\". If you click on it you see that the dimension of the layer is: (batch_size, sequence_length, 764).\n",
    "\n",
    "- Batch size is the size of the batch that passed through the model\n",
    "- Sequence length is the max number of tokens in the batch\n",
    "- 764 is the size of the token embeddings\n",
    "\n",
    "We can use onnx to export the model we just initialized and if we visualize it in netron we see that a linear head has been added on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is a sample input for ONNX export.\", return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                                          # Model to export\n",
    "    (inputs[\"input_ids\"].cuda(), inputs[\"attention_mask\"].cuda()), # Input example (tuple)\n",
    "    \"distilbert_model_binary_classification.onnx\",                         # Path to save ONNX model\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],     # Names for the inputs\n",
    "    output_names=[\"output\"],                         # Names for the outputs\n",
    "    dynamic_axes={                                   # Dynamic axes for variable sequence lengths\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=14                                # ONNX opset version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the transformer model using the transformers library Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "test_data = test_data.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids is the list of token ids in the input text, and attention_mask is the attention mask vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpinosio/miniconda3/envs/teaching/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the transformer. This will take a bit of time to run (note: if you are running on gpu, you can use `nvidia-smi` to monitor gpu utilization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.416138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 01:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41613781452178955,\n",
       " 'eval_runtime': 9.3628,\n",
       " 'eval_samples_per_second': 53.403,\n",
       " 'eval_steps_per_second': 6.729,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above is training the transformer model and updating all the model parameters with backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: distilbert.embeddings.word_embeddings.weight | Shape: torch.Size([30522, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.position_embeddings.weight | Shape: torch.Size([512, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.LayerNorm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.LayerNorm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: pre_classifier.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: pre_classifier.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: classifier.weight | Shape: torch.Size([2, 768]) | Requires grad: True\n",
      "Parameter name: classifier.bias | Shape: torch.Size([2]) | Requires grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name} | Shape: {param.shape} | Requires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train only the parameters for the linear head by setting the other parameters to not require gradients. This is done by setting the requires_grad attribute to False for the parameters we don't want to update. This will make the training faster. Typically we do not want to train the transformer parameters because they are already trained on a large corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.distilbert.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.424694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42469412088394165,\n",
       " 'eval_runtime': 9.2942,\n",
       " 'eval_samples_per_second': 53.797,\n",
       " 'eval_steps_per_second': 6.778,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict with the trained model on new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"this is really bad\", \"this is really good\"], return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokens[\"input_ids\"].cuda(), attention_mask=tokens[\"attention_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this model is the raw logits. We can use the softmax function to convert the logits to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6164, -3.0171],\n",
       "        [-2.0582,  2.1609]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0 | Probability: tensor([0.9964, 0.0036], device='cuda:0')\n",
      "Predicted label: neg\n",
      "Prediction: 1 | Probability: tensor([0.0145, 0.9855], device='cuda:0')\n",
      "Predicted label: pos\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(outputs.logits, dim=1)\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Prediction: {predictions[i]} | Probability: {torch.softmax(outputs.logits[i], dim=0)}\")\n",
    "    print(f\"Predicted label: {dataset['train'].features['label'].int2str(predictions[i].item())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transformers library has a pipeline class that makes it easy to use the model for inference. In this way, we can use the model to predict the sentiment of new text without having to write the code to tokenize the text, pass it through the model, and convert the logits to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998807907104492}]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for sentiment analysis using an already pre-trained sentiment analysis model\n",
    "# note: using distilbert here would be very bad because the model is not trained for sentiment analysis\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Analyze the sentiment of a sample text\n",
    "result = sentiment_analysis(\"This is a fantastic movie. I love it!\")\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great, but how does it work in the background? we can try to implement a (simple) transformer model from scratch to understand how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size needs to be divisible by heads\"\n",
    "        \n",
    "        # Define linear transformations for Q, K, V\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        print(\"values shape:\", values.shape)\n",
    "        \n",
    "        # Apply the linear transformations\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "        \n",
    "        print(\"values shape:\", values.shape)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        print(\"values shape:\", values.shape)\n",
    "        \n",
    "        # Compute the attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
    "\n",
    "        # Compute the weighted sum of the values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, forward_expansion, dropout, vocab_size, max_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size) # this will initialize the word embeddings for the tokens\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Classification head with 2 output units (positive, negative)\n",
    "        # in the example above it was added on top of distilbert\n",
    "        self.fc_out = nn.Linear(embed_size, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask): # here we pass the token ids and the mask tensors\n",
    "        N, seq_length = x.shape # N is the batch size, while seq_length is the length of the sequence (how many tokens)\n",
    "        print(\"input shape:\", x.shape)\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device) # this is just the position tensor, it is neeeded for the positional encoding (to know the position of the token in the sequence)\n",
    "        \n",
    "        print(\"position tensor shape:\", positions.shape)\n",
    "        \n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        \n",
    "        print(\"after first dropout shape\", out.shape)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # passing the same vectors three times seem odd, but remember that the self-attention mechanism\n",
    "            # multiplies each batch vector with three different weight matrices\n",
    "            out = layer(out, out, out, mask)\n",
    "            print(\"transformer block output shape\",out.shape)\n",
    "        \n",
    "        # Average pooling over the sequence length.\n",
    "        # not completely accurate as one would have to mask the padding tokens but good enough for now\n",
    "        out = out.mean(dim=1)\n",
    "        \n",
    "        print(\"after mean pooling shape\", out.shape)\n",
    "        \n",
    "        # project the output to the classification head\n",
    "        out = self.fc_out(out)\n",
    "        print(\"final shape\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 128])\n",
      "position tensor shape: torch.Size([1, 128])\n",
      "after first dropout shape torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 8, 96])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 8, 96])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 8, 96])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 8, 96])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 8, 96])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 768])\n",
      "values shape: torch.Size([1, 128, 8, 96])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "after mean pooling shape torch.Size([1, 768])\n",
      "final shape torch.Size([1, 2])\n",
      "Predicted sentiment: negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3201, -0.1206]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love using transformers library!\"\n",
    "tokens = tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# convert tokens to tensor format\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "\n",
    "# initialize the Transformer model\n",
    "model = Transformer(embed_size=768, heads=8, num_layers=6, forward_expansion=4, dropout=0.1, vocab_size=tokenizer.vocab_size, max_length=128)\n",
    "\n",
    "# pass the tensor through the Transformer model\n",
    "output = model(input_ids, attention_mask)\n",
    "\n",
    "# interpret the output. This will be bad as the classification head is not trained.\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "print(f\"Predicted sentiment: {class_names[predicted_class]}\")\n",
    "\n",
    "# however, the output has the gradients attached to it, so we can use it to train the model. This is because pytorch is able to keep track of the gradients of the model, i.e., \n",
    "# it is able to compute the gradients of the loss function with respect to the model parameters from the model definition. This is done by the autograd package in pytorch.\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.stackexchange.com/questions/41477/why-in-multi-head-attention-implementation-should-we-use-3-linear-layers-for-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddb_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
