{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpinosio/miniconda3/envs/ddb_tutorial/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch # pytorch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the imbd dataset for sentiment classification. It already has labels (1 for positive, 0 for negative sentiment).\n",
    "- See it here: https://huggingface.co/datasets/stanfordnlp/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(2000))  # Limit for faster execution\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\" # model name from huggingface repo\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # load the tokenizer model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # load the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above warning is normal. What is happening here is that we are taking the distilbert model, which is just a (smaller) bert model whose final layer is the last encoding layer (no linear layer). Hence, this model just computes vector embeddings for the tokens in a sequence. Then, with AutoModelForSequenceClassification we are putting a linear head on top to take the tokens, mean pool them, and classify the resulting vector into \"positive\" or \"negative\".\n",
    "\n",
    "We can visualize this model (using an [onnx](https://onnx.ai/) version) using [netron](https://github.com/lutzroeder/netron): see [here](https://netron.app/?url=https://huggingface.co/onnxport/distilbert-base-uncased-onnx/blob/main/model.onnx).\n",
    "\n",
    "Note how the last node (the output) is named \"last_hidden_layer\". If you click on it you see that the dimension of the layer is: (batch_size, sequence_length, 764).\n",
    "\n",
    "- Batch size is the size of the batch that passed through the model\n",
    "- Sequence length is the max number of tokens in the batch\n",
    "- 764 is the size of the token embeddings\n",
    "\n",
    "We can use onnx to export the model we just initialized and if we visualize it in netron we see that a linear head has been added on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is a sample input for ONNX export.\", return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                                          # Model to export\n",
    "    (inputs[\"input_ids\"].cuda(), inputs[\"attention_mask\"].cuda()), # Input example (tuple)\n",
    "    \"distilbert_model_binary_classification.onnx\",                         # Path to save ONNX model\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],     # Names for the inputs\n",
    "    output_names=[\"output\"],                         # Names for the outputs\n",
    "    dynamic_axes={                                   # Dynamic axes for variable sequence lengths\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=14                                # ONNX opset version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the transformer model using the transformers library Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "test_data = test_data.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids is the list of token ids in the input text, and attention_mask is the attention mask vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpinosio/miniconda3/envs/ddb_tutorial/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the transformer. This will take a bit of time to run (note: if you are running on gpu, you can use `nvidia-smi` to monitor gpu utilization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 02:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 13:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.46240007877349854,\n",
       " 'eval_runtime': 7.6302,\n",
       " 'eval_samples_per_second': 65.529,\n",
       " 'eval_steps_per_second': 8.257,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above is training the transformer model and updating all the model parameters with backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: distilbert.embeddings.word_embeddings.weight | Shape: torch.Size([30522, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.position_embeddings.weight | Shape: torch.Size([512, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.LayerNorm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.embeddings.LayerNorm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.0.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.1.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.2.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.3.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.4.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.q_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.q_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.k_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.k_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.v_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.v_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.out_lin.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.attention.out_lin.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.sa_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.sa_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin1.weight | Shape: torch.Size([3072, 768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin1.bias | Shape: torch.Size([3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin2.weight | Shape: torch.Size([768, 3072]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.ffn.lin2.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.output_layer_norm.weight | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: distilbert.transformer.layer.5.output_layer_norm.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: pre_classifier.weight | Shape: torch.Size([768, 768]) | Requires grad: True\n",
      "Parameter name: pre_classifier.bias | Shape: torch.Size([768]) | Requires grad: True\n",
      "Parameter name: classifier.weight | Shape: torch.Size([2, 768]) | Requires grad: True\n",
      "Parameter name: classifier.bias | Shape: torch.Size([2]) | Requires grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name} | Shape: {param.shape} | Requires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train only the parameters for the linear head by setting the other parameters to not require gradients. This is done by setting the requires_grad attribute to False for the parameters we don't want to update. This will make the training faster. Typically we do not want to train the transformer parameters because they are already trained on a large corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.distilbert.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.436514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4365137219429016,\n",
       " 'eval_runtime': 8.9083,\n",
       " 'eval_samples_per_second': 56.127,\n",
       " 'eval_steps_per_second': 7.072,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict with the trained model on new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"this is really bad\", \"this is really good\"], return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokens[\"input_ids\"].cuda(), attention_mask=tokens[\"attention_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this model is the raw logits. We can use the softmax function to convert the logits to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6475, -2.3122],\n",
       "        [-2.4580,  2.6524]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0 | Probability: tensor([0.9930, 0.0070], device='cuda:0')\n",
      "Predicted label: neg\n",
      "Prediction: 1 | Probability: tensor([0.0060, 0.9940], device='cuda:0')\n",
      "Predicted label: pos\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(outputs.logits, dim=1)\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Prediction: {predictions[i]} | Probability: {torch.softmax(outputs.logits[i], dim=0)}\")\n",
    "    print(f\"Predicted label: {dataset['train'].features['label'].int2str(predictions[i].item())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transformers library has a pipeline class that makes it easy to use the model for inference. In this way, we can use the model to predict the sentiment of new text without having to write the code to tokenize the text, pass it through the model, and convert the logits to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998807907104492}]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for sentiment analysis using an already pre-trained sentiment analysis model\n",
    "# note: using distilbert here would be very bad because the model is not trained for sentiment analysis\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Analyze the sentiment of a sample text\n",
    "result = sentiment_analysis(\"This is a fantastic movie. I love it!\")\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great, but how does it work in the background? we can try to implement a (simple) transformer model from scratch using pytorch to understand how it works!\n",
    "\n",
    "Pytorch is (one of) the main frameworks to train deep learning models. It is a low-level library that provides a lot of flexibility to build custom models. The transformers library is built on top of pytorch and provides a high-level API to train transformer models.\n",
    "\n",
    "In the code below, the transformer model is a class that inherits from the nn.Module class. The model has an embedding layer, a transformer encoder, and a linear head. The forward method of the model takes the input_ids and attention_mask as input and returns the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size needs to be divisible by heads\"\n",
    "        \n",
    "        # Define linear transformations for Q, K, V\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        print(\"attention forward pass: values shape:\", values.shape)\n",
    "        \n",
    "        # Apply the linear transformations\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "        \n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        print(\"values shape after splitting into the attention heads:\", values.shape)\n",
    "        \n",
    "        print(\"computing the attention scores\")\n",
    "        \n",
    "        # Compute the attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
    "\n",
    "        # Compute the weighted sum of the values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        print(\"dimensions of the final attention scores:\", out.shape)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads) # embed_size is the size of the word embeddings, heads is the number of attention heads we have\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        print(\"calculating self-attention\")\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, forward_expansion, dropout, vocab_size, max_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size) # this will initialize the word embeddings for the tokens\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Classification head with 2 output units (positive, negative)\n",
    "        # in the example above it was added on top of distilbert\n",
    "        self.fc_out = nn.Linear(embed_size, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask): # here we pass the token ids and the mask tensors\n",
    "        N, seq_length = x.shape # N is the batch size, while seq_length is the length of the sequence (how many tokens)\n",
    "        print(\"input shape:\", x.shape)\n",
    "        \n",
    "        # this is just the position tensor, it is neeeded for the positional encoding (to know the position of the token in the sequence)\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        print(\"position tensor shape:\", positions.shape)\n",
    "        \n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        \n",
    "        print(\"shape of output after first dropout\", out.shape, \" the last dimension is the embedding size of a token\")\n",
    "        \n",
    "        counter = 1\n",
    "\n",
    "        for layer in self.layers:\n",
    "            print(\"running transformer block: \", counter)\n",
    "            # passing the same vectors three times seem odd, but remember that the self-attention mechanism\n",
    "            # multiplies each batch vector with three different weight matrices\n",
    "            out = layer(out, out, out, mask)\n",
    "            print(\"transformer block output shape\", out.shape)\n",
    "            print(\"transformer block \", counter, \" done\")\n",
    "            counter += 1\n",
    "        \n",
    "        # Average pooling over the sequence length.\n",
    "        # not completely accurate as one would have to mask the padding tokens but good enough for now\n",
    "        out = out.mean(dim=1)\n",
    "        \n",
    "        print(\"after mean pooling shape\", out.shape)\n",
    "        \n",
    "        # project the output to the classification head\n",
    "        out = self.fc_out(out)\n",
    "        print(\"final shape\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 128])\n",
      "position tensor shape: torch.Size([1, 128])\n",
      "shape of output after first dropout torch.Size([1, 128, 768])  the last dimension is the embedding size of a token\n",
      "running transformer block:  1\n",
      "calculating self-attention\n",
      "attention forward pass: values shape: torch.Size([1, 128, 768])\n",
      "values shape after splitting into the attention heads: torch.Size([1, 128, 8, 96])\n",
      "computing the attention scores\n",
      "dimensions of the final attention scores: torch.Size([1, 128, 768])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "transformer block  1  done\n",
      "running transformer block:  2\n",
      "calculating self-attention\n",
      "attention forward pass: values shape: torch.Size([1, 128, 768])\n",
      "values shape after splitting into the attention heads: torch.Size([1, 128, 8, 96])\n",
      "computing the attention scores\n",
      "dimensions of the final attention scores: torch.Size([1, 128, 768])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "transformer block  2  done\n",
      "running transformer block:  3\n",
      "calculating self-attention\n",
      "attention forward pass: values shape: torch.Size([1, 128, 768])\n",
      "values shape after splitting into the attention heads: torch.Size([1, 128, 8, 96])\n",
      "computing the attention scores\n",
      "dimensions of the final attention scores: torch.Size([1, 128, 768])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "transformer block  3  done\n",
      "running transformer block:  4\n",
      "calculating self-attention\n",
      "attention forward pass: values shape: torch.Size([1, 128, 768])\n",
      "values shape after splitting into the attention heads: torch.Size([1, 128, 8, 96])\n",
      "computing the attention scores\n",
      "dimensions of the final attention scores: torch.Size([1, 128, 768])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "transformer block  4  done\n",
      "running transformer block:  5\n",
      "calculating self-attention\n",
      "attention forward pass: values shape: torch.Size([1, 128, 768])\n",
      "values shape after splitting into the attention heads: torch.Size([1, 128, 8, 96])\n",
      "computing the attention scores\n",
      "dimensions of the final attention scores: torch.Size([1, 128, 768])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "transformer block  5  done\n",
      "running transformer block:  6\n",
      "calculating self-attention\n",
      "attention forward pass: values shape: torch.Size([1, 128, 768])\n",
      "values shape after splitting into the attention heads: torch.Size([1, 128, 8, 96])\n",
      "computing the attention scores\n",
      "dimensions of the final attention scores: torch.Size([1, 128, 768])\n",
      "transformer block output shape torch.Size([1, 128, 768])\n",
      "transformer block  6  done\n",
      "after mean pooling shape torch.Size([1, 768])\n",
      "final shape torch.Size([1, 2])\n",
      "Predicted sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love using transformers library!\"\n",
    "tokens = tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# convert tokens to tensor format\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "\n",
    "# initialize the Transformer model with 8 heads and embed_size of 768\n",
    "model = Transformer(embed_size=768, heads=8, num_layers=6, forward_expansion=4, dropout=0.1, vocab_size=tokenizer.vocab_size, max_length=128)\n",
    "\n",
    "# pass the tensor through the Transformer model\n",
    "output = model(input_ids, attention_mask)\n",
    "\n",
    "# interpret the output. This will be bad as the classification head is not trained of course.\n",
    "# Also note that the output here are the raw logits.\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "print(f\"Predicted sentiment: {class_names[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the attention layers, it might be confusing how we are first applying the linear transformations on the whole input and then splitting it into heads for the calculation of the weights. This is just an optimization trick to avoid applying the linear transformations multiple times for each head. The linear transformations are applied only once and then the output is split into heads. It's equivalent to what we have seen in class.\n",
    "\n",
    "\n",
    "https://ai.stackexchange.com/questions/41477/why-in-multi-head-attention-implementation-should-we-use-3-linear-layers-for-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted outputs of the model above will be very bad because the weights of the classification head have not been trained, differently from the model we trained using transformers.\n",
    "\n",
    "However, the pytorch output has the gradients attached to it, so we could use it to train the model. This is because pytorch is able to keep track of the gradients of the model, i.e., \n",
    "it is able to compute the gradients of the loss function with respect to the model parameters from the model definition. This is done by the autograd package in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3172, -0.3496]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing needed to train a pytorch model like this one is to write a training loop that computes the loss for training examples and uses the gradients to update the model parameters. This is done by the optimizer object in pytorch. For more details on training directly with pytorch see the pytorch documentation: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddb_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
