{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ddb_ml_analysis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22886844d03d05437dbb143181697e7f50898b0638617c0ae6b2977daba4ec36"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TOPIC MODELLING WITH NMF\n",
    "\n",
    "In this notebook we are going to show how to do topic modelling with NMF. The goal is to learn how we can extract topics from natural language text using matrix factorization. These topics can then be used for various purposed, for instance, to extract features from a text description that can be used as independent features in a supervised modelling problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In the following, we will use both sklearn (for the NMF model) and the spacy NLP library (for NLP transformations).\n",
    "In particular, spacy comes with pretrained language models (they are essentially mostly neural networks) which can be used for a variety of purposes, such as, tokenization, entity recognition, and so forth. These models must be installed individually in the conda environment.\n",
    "\n",
    "To install spacy and the english language model in your conda environment, you can use the following commands:\n",
    "\n",
    "- pip install spacy\n",
    "- python -m spacy download en_core_web_sm\n",
    "\n",
    "You can find more information on spacy at https://spacy.io/usage/spacy-101\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"../dataset/news-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1103658      20171231  the ashes smiths warners near miss liven up bo...\n",
       "1103659      20171231            timelapse: brisbanes new year fireworks\n",
       "1103660      20171231           what 2017 meant to the kids of australia\n",
       "1103661      20171231   what the papodopoulos meeting may mean for ausus\n",
       "1103662      20171231  who is george papadopoulos the former trump ca...\n",
       "\n",
       "[1103663 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>publish_date</th>\n      <th>headline_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20030219</td>\n      <td>aba decides against community broadcasting lic...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20030219</td>\n      <td>act fire witnesses must be aware of defamation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20030219</td>\n      <td>a g calls for infrastructure protection summit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20030219</td>\n      <td>air nz staff in aust strike for pay rise</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20030219</td>\n      <td>air nz strike to affect australian travellers</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1103658</th>\n      <td>20171231</td>\n      <td>the ashes smiths warners near miss liven up bo...</td>\n    </tr>\n    <tr>\n      <th>1103659</th>\n      <td>20171231</td>\n      <td>timelapse: brisbanes new year fireworks</td>\n    </tr>\n    <tr>\n      <th>1103660</th>\n      <td>20171231</td>\n      <td>what 2017 meant to the kids of australia</td>\n    </tr>\n    <tr>\n      <th>1103661</th>\n      <td>20171231</td>\n      <td>what the papodopoulos meeting may mean for ausus</td>\n    </tr>\n    <tr>\n      <th>1103662</th>\n      <td>20171231</td>\n      <td>who is george papadopoulos the former trump ca...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1103663 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "source": [
    "## Data cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we take the actual texts as a vector\n",
    "text = news_df['headline_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# next, this command will load the english language model of spacy, which we have previously installed\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "type(nlp)"
   ]
  },
  {
   "source": [
    "The first thing we need to do is to transform the natural language sentences in the text vector into lists of tokens. A token can be defined as a string of contiguous characters between two spaces, or between a space and punctuation marks, but there are a lot of exceptions to this definition depending on the language used. Spacy NLP models have been trained to tokenize text for specific languages. Below, we see an example for English."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'aba decides against community broadcasting licence'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aba\ndecides\nagainst\ncommunity\nbroadcasting\nlicence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"It's cold in winter in N.Y\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nIt\n's\ncold\nin\nwinter\nin\nN.Y\n"
     ]
    }
   ],
   "source": [
    "# we can use the nlp object to tokenize a string into individual tokens:\n",
    "text[0]\n",
    "\n",
    "for token in nlp(text[0]):\n",
    "    print(token)\n",
    "\n",
    "# note that the spacy language model has been trained to recognize exceptions to tokenization rules\n",
    "# for instance, in the below, N.Y. is treated as a single token\n",
    "\"It's cold in winter in N.Y\"\n",
    "print(\"\\n\")\n",
    "tokenization_example = nlp(\"It's cold in winter in N.Y\")\n",
    "for token in tokenization_example:\n",
    "    print(token)"
   ]
  },
  {
   "source": [
    "Spacy tokenizer can be customized if needed. See https://spacy.io/usage/linguistic-features#tokenization for how to do this. Note that for e.g. the funda case, you will need the Dutch language model to do this: see https://spacy.io/models/nl."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One of the features of spacy is that the application of the nlp() function (i.e., the \"language\" object) does not only do tokenization, but applies a full pre-trained end-to-end natural language processing pipeline to the text. For instance, when nlp is called above, spacy applies the following operations:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/spacy_nlp_pipeline.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Note that spacy will first take the text; then turn it into a list of tokens (a **doc** object); and then use a variety of pre-trained statistical model to annotate these tokens with additional information, to **extract** structured information from the text. For instance, the NER model in the pipeline attempts to identify tokens that refer to specific entities, such as people, organizations, or cities; we call this **named entity recognition**, and it is a fundamental task in NLP."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(winter, N.Y)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spacy recognized entity: winter, which has been labelled as: DATE\nSpacy recognized entity: N.Y, which has been labelled as: GPE\n"
     ]
    }
   ],
   "source": [
    "# the entities that spacy recognized in our tokenization example\n",
    "tokenization_example.ents\n",
    "for i in tokenization_example.ents:\n",
    "    print(\"Spacy recognized entity: {}, which has been labelled as: {}\".format(i, i.label_)) # GPE stands for geopolitical entity"
   ]
  },
  {
   "source": [
    "In our application, however, we will only need the tokenization feature; we are not interested in the other elements of the pipeline (the parser, NER, and so forth) (the parser is the pipeline component that identifies part of speech, i.e. it does part of speech tagging, such as identifying nouns and verbs and **negations**). Hence we will disable the other default features of the spacy pipeline and only run the rule-based tokenizer using nlp.make_doc. This will also be a lot faster than using the statistical models.\n",
    "We can now tokenize all the documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform tokenization\n",
    "docs = [nlp.make_doc(x) for x in text]"
   ]
  },
  {
   "source": [
    "Next, we need to remove the stopwords from the tokenized documents. Spacy comes with a built in list of stopwords:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'twenty', 'also', 'since', 'whom', 'whence', 'side', '’m', 'perhaps', 'should', 'toward', 'bottom', 'therefore', 'am', 'yet', 'around', 'made', 'keep', 'be', 'former', 'hence', 'or', 'per', 'into', 'anywhere', 'been', 'which', 'third', 'every', 'whenever', 'show', 'herein', 'anyhow', 'get', '‘ve', 'doing', 'nevertheless', 'what', 'their', 'wherever', 'itself', 'they', 'thereby', 'otherwise', 'them', 'much', 'me', 'least', 'often', 'else', 'cannot', 'even', 'while', 'below', 'own', 'hundred', 'front', 'after', 'moreover', 'yourselves', 'ten', 'amount', 'becomes', 'due', 'you', 'besides', 'can', 'eight', 'not', 'his', 'few', 'nothing', 'might', 'against', 'everything', 'last', 'did', 'most', 'the', 'full', 'thru', '‘m', 'one', 'becoming', 'once', \"'re\", 'each', 'used', 'have', 'in', 'by', 'at', 're', 'herself', 'towards', 'thereafter', 'up', 'themselves', 'fifty', 'across', 'whither', \"'m\", 'somewhere', 'before', 'everyone', 'using', 'a', 'some', 'whose', \"n't\", 'further', 'we', 'quite', 'over', 'onto', 'ourselves', '‘d', 'during', 'really', 'whereupon', 'hereby', 'many', 'except', 'above', 'do', 'others', 'without', 'via', 'may', 'yours', 'fifteen', 'never', 'say', 'thus', 'because', 'beyond', 'well', 'ca', 'if', 'with', 'from', 'take', 'had', 'whole', 'ours', 'myself', 'top', 'several', 'other', 'any', 'unless', 'so', 'part', 'upon', 'your', 'these', 'either', 'rather', 'such', 'whoever', 'five', 'serious', 'afterwards', 'mostly', 'whereby', 'her', 'there', 'why', 'who', 'just', '’s', 'to', 'has', 'where', 'somehow', 'too', 'must', 'three', '‘s', 'thence', 'thereupon', \"'ll\", 'sometime', 'this', '‘ll', 'various', 'although', 'still', 'ever', 'behind', 'therein', 'less', 'six', 'could', 'until', 'already', 'enough', 'my', 'latterly', 'how', 'four', 'wherein', 'between', 'very', 'almost', 'will', 'together', 'seems', 'seemed', 'its', 'amongst', 'became', 'no', 'seem', 'within', 'latter', 'only', 'put', 'neither', 'make', '’re', 'now', 'regarding', 'he', 'than', 'sixty', 'elsewhere', 'an', 'that', 'empty', 'namely', 'nowhere', 'out', 'whereas', 'both', 'twelve', 'sometimes', \"'ve\", 'beside', 'back', 'and', 'go', 'himself', '’ll', 'anyway', 'n’t', 'nine', 'always', 'along', 'n‘t', 'more', 'everywhere', 'give', 'i', 'under', 'forty', '’d', 'done', 'alone', '‘re', 'nor', 'is', 'when', 'same', 'please', 'whereafter', 'next', 'those', 'all', 'however', 'meanwhile', \"'d\", 'whatever', 'as', 'yourself', 'but', 'anyone', 'first', '’ve', 'about', 'for', \"'s\", 'would', 'our', 'name', 'on', 'him', 'among', 'formerly', 'she', 'call', 'eleven', 'see', 'it', 'move', 'hers', 'someone', 'hereupon', 'mine', 'off', 'was', 'though', 'are', 'become', 'two', 'hereafter', 'down', 'here', 'indeed', 'does', 'being', 'something', 'then', 'none', 'whether', 'through', 'seeming', 'of', 'were', 'again', 'noone', 'another', 'nobody', 'anything', 'throughout', 'us', 'beforehand'}\n"
     ]
    }
   ],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "print(stopwords)"
   ]
  },
  {
   "source": [
    "We can add new stopwords to the list, like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.Defaults.stop_words.add(\"my_new_stopword\")"
   ]
  },
  {
   "source": [
    "Stop words, punctuation, and other things like numbers/digits do not encode semantic content and should be removed before doing topic modelling. We can do so easily as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "it's a match\n"
     ]
    }
   ],
   "source": [
    "# the following lines of code illustrate how to check if a string is all composed of digits and punctuation using regex\n",
    "digit_re = re.compile('^([0-9]|[\\\\.,])*$')\n",
    "if re.match(digit_re, \"1234,500.3\"):\n",
    "    print(\"it's a match\")\n",
    "else:\n",
    "    print(\"it's not a match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cleaned = []\n",
    "for doc in docs:\n",
    "    new_tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not re.match(digit_re, token.text):\n",
    "            new_tokens.append(token.text) # append the text only because we don't need the spacy tokens anymore from now on\n",
    "    tokens_cleaned.append(new_tokens)"
   ]
  },
  {
   "source": [
    "Note two things:\n",
    "\n",
    "1. after we remove the stopwords like in the above, the tokens_cleaned is not a list of doc objects anymore; it is now a list of lists of token objects. This is because the principle of spacy is that you should always be able to reconstruct the original text from a doc object (i.e., a doc object only parses and adds new structure over the natural language text)\n",
    "2. if you add your own stopwords, then you will need to do that BEFORE running the tokenization with make_doc for it to be picked up"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['aba', 'decides', 'community', 'broadcasting', 'licence'],\n",
       " ['act', 'fire', 'witnesses', 'aware', 'defamation'],\n",
       " ['g', 'calls', 'infrastructure', 'protection', 'summit'],\n",
       " ['air', 'nz', 'staff', 'aust', 'strike', 'pay', 'rise'],\n",
       " ['air', 'nz', 'strike', 'affect', 'australian', 'travellers']]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokens_cleaned[0:5]"
   ]
  },
  {
   "source": [
    "Now that we have tokenized and cleaned the documents (which in our case are sentences headlines), we are going to use sklearn CountVectorizer to create a big matrix of dimension N x M, where N are the individual documents (the healdines), and the columns M of the matrix are the individual words that occur in the *corpus* (i.e., in the whole list of documents). The values of the matrix cells V_ij will be the occurrence counts of word j in document i."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to pass a dummy function to the tokenizer and preprocessor parameters of count vectorizer because we already calculated our tokens\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    )\n",
    "\n",
    "frequency_matrix = count_vectorizer.fit_transform(tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "type(frequency_matrix)"
   ]
  },
  {
   "source": [
    "Note that sklearn stores the frequency matrix as a sparse matrix, which is more efficient for 0 inflated matrices like the one above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['$', \"'em\", '0.2pc', '0.6pc', '01pc', '026pc', '02pc', '035pc',\n",
       "       '03pc', '03rd', '04pc', '05pc', '06pc', '083pc', '08s', '09pc',\n",
       "       '1.1b', '1.26b', '1.2b', '1.2pc', '1.30am', '1.3b', '1.3bn',\n",
       "       '1.4b', '1.59b', '1.5b', '1.5pc', '1.6b', '1.7b', '1.8b', '1.9bn',\n",
       "       '10.5pc', '10.7pc', '10.9pc', '10000th', '10000yo', '1000cc',\n",
       "       '1000k', '1000kms', '1000pc', '1000s', '1000th', '1000yo', '100am',\n",
       "       '100b', '100k', '100kgs', '100kms', '100kph', '100mi'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "source": [
    "You can see that although we removed the digits, still, there are words left in the corpus that have little semantic meaning in the context of a bag of word model, such as 100kms and so forth. One might want to remove these from the corpus as well using specific rules or lists of words to remove. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['withold', 'witholding', 'withought', 'withour', 'withstand',\n",
       "       'withstanding', 'withstands', 'withstood', 'witih', 'witkop',\n",
       "       'witnes', 'witness', 'witnesse', 'witnessed', 'witnesses',\n",
       "       'witnessing', 'witnesss', 'witnsses', 'wits', 'witsunday', 'witt',\n",
       "       'wittenoom', 'wittenooms', 'wittner', 'witton', 'witts', 'witty',\n",
       "       'witzig', 'wivenhoe', 'wives', 'wiwa', 'wiya', 'wiz', 'wizard',\n",
       "       'wizardry', 'wizards', 'wizkid', 'wj', 'wjdap', 'wk', 'wk2',\n",
       "       'wknd', 'wks', 'wladimi', 'wladimir', 'wleague', 'wlecome',\n",
       "       'wlefare', 'wlhd', 'wlliams'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()[-2000:-1950]"
   ]
  },
  {
   "source": [
    "Note also that words like, e.g., \"withold\" and \"witholding\" are considered as separate words in this analysis. This is also not perfect, because the semantic content of the words \"withold\" and \"witholding\" is the same, and we would like in principle to count them as one word. To do so, one could apply **lemmatization** to identify the underlying lemma of the two words (in this case, \"withold\"), and count the occurrences of the lemma itself rather than the individual words. Spacy has a lemmatization component: https://spacy.io/api/lemmatizer that can be used in a nlp pipeline for this purpose, but we won't worry about it for this demo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another thing that can be useful to reduce the dimensionality of the matrix it to pass the max_feature argument to the count vectorizer. Then, sklearn will create a matrix only with the top max_feature features in terms of frequency in the document. Here, we are going to use max_features 5000. However, this parameter needs to be adjusted based on data analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    max_features=5000 # add the max_features argument\n",
    "    )\n",
    "\n",
    "frequency_matrix = count_vectorizer.fit_transform(tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1103663x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4555971 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "frequency_matrix"
   ]
  },
  {
   "source": [
    "Next, we apply the tf-idf transformer to turn the frequency matrix into a tf-idf matrix. Tf-idf stands for: *term frequency inverse document frequency*. It is a statistical measure of how relevant a word is to a given document, in the context of a collection of documents.\n",
    "\n",
    "It is calculated as: **frequency of a word in a document * inverse document frequency of the word across all documents**.\n",
    "\n",
    "The inverse document frequency encodes how rare a word is in the whole dataset. The closest to 0 the inverse document frequency is, the more common the word is. It is calculated by taking the total number of documents, dividing it by the number of documents containing the word, and then taking the logarithm.\n",
    "\n",
    "The idea is that if a word has high idf, then it is a rare word in the corpus; hence, is likely to carry a lot of the semantics content of the document compared to words that are very common. E.g., a word like house in the funda dataset would occur very often and be less meaningful than a word like \"garden\" or \"school\" or \"dakkapel\", hence we want to give higher weight to words in a document that are infrequent in general.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/tf_idf.jpeg\" width=\"500\">\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd_idf_trans = TfidfTransformer()\n",
    "tf_idf_matrix = tfd_idf_trans.fit_transform(frequency_matrix)"
   ]
  },
  {
   "source": [
    "The tf idf matrix is also stored as a sparse matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1103663x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4555971 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "tf_idf_matrix"
   ]
  },
  {
   "source": [
    "We now use the NMF model from sklearn (which is actually implemented as a transformer rather than an estimator) to decompose our tf-idf matrix into two: a document x topics matrix W, and a topics x words matrix H. Thus, the model finds two matrices W, H, such that tf_idf_matrix ~ WH.\n",
    "\n",
    "Note the sign ~ in the equation above, and not the sign =. This is because the two decomposing matrices W, H will not return the original tf_idf_matrix when multiplied together; rather, they will return a new matrix that approximates the original tf_idf matrix with an error E.\n",
    "\n",
    "One important caveat of this model is that the number of topics to be discovered (the n_components parameter in the NMF model), i.e., the number of columns of W and the number of rows of H, is a **hyperparameter**; i.e., it needs to be specified when the model is instantiated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NMF(init='nndsvd', n_components=10)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# we use 10 as the n_components here\n",
    "model = NMF(n_components=10, init='nndsvd')\n",
    "\n",
    "# fit the model\n",
    "model.fit(X=tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 5.59881810e-06, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.01220539e-02, 2.61696942e-03, 8.76515833e-03, ...,\n",
       "        8.20097637e-04, 0.00000000e+00, 6.76022141e-03],\n",
       "       [3.38853033e-03, 9.90211870e-04, 3.49371009e-03, ...,\n",
       "        5.81247911e-03, 2.49112509e-03, 0.00000000e+00],\n",
       "       ...,\n",
       "       [3.70511399e-03, 2.20310474e-04, 0.00000000e+00, ...,\n",
       "        8.31889312e-03, 3.40713921e-03, 3.98590572e-04],\n",
       "       [1.15192810e-02, 1.83898953e-05, 3.90650924e-04, ...,\n",
       "        1.37729671e-02, 7.12337185e-03, 5.91310975e-03],\n",
       "       [2.27368217e-03, 3.85378359e-04, 3.48036846e-02, ...,\n",
       "        7.26846073e-03, 0.00000000e+00, 8.73785186e-03]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# the model.components_ matrix is the H matrix mapping topics to word weights\n",
    "model.components_\n",
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, n_top_words):\n",
    "    '''This function takes a fitted NMF factorization model, and a n_top_words parameter.\n",
    "    It then produces a dataframe where the columns are the topics that have been learned, and the\n",
    "    rows are the top words that define the topic, ranked by their coefficients.\n",
    "    '''\n",
    "\n",
    "    feature_names = count_vectorizer.get_feature_names()\n",
    "    d = {}\n",
    "    for i in range(model.n_components):\n",
    "        # model.components_ matrix contains the matrix H where topics are \n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feature_names[key] for key in words_ids]\n",
    "        d['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Topic # 01 Topic # 02   Topic # 03 Topic # 04  Topic # 05     Topic # 06  \\\n",
       "0   interview        man       police        new        says            abc   \n",
       "1     michael    charged        probe    zealand     council           news   \n",
       "2    extended     murder  investigate       laws        govt          rural   \n",
       "3       david     jailed      missing       year        plan       business   \n",
       "4        john    missing       search   hospital       water        weather   \n",
       "5       james   stabbing        death       home          nt          sport   \n",
       "6         nrl     guilty         hunt       york   australia         market   \n",
       "7         ben      found      officer     centre      health       national   \n",
       "8        matt   arrested     shooting       deal       urged       analysis   \n",
       "9       smith      death         seek      years      report  entertainment   \n",
       "10     andrew     sydney       arrest       gets     funding          talks   \n",
       "11       luke    assault         find      opens       boost         speaks   \n",
       "12      scott     attack         drug  president       calls       reporter   \n",
       "13     nathan       dies      assault      chief  government          share   \n",
       "14       ivan      child      station        set         qld       exchange   \n",
       "15       mark     pleads         body      named    minister           quiz   \n",
       "16      chris   shooting         help       life           m     australian   \n",
       "17        tim        sex        fatal      south          mp            qld   \n",
       "18      peter     search        chase        ceo       labor      breakfast   \n",
       "19      steve       shot          car      rules      budget         friday   \n",
       "\n",
       "   Topic # 07 Topic # 08  Topic # 09 Topic # 10  \n",
       "0       court    country        fire      crash  \n",
       "1     accused       hour       house        car  \n",
       "2        face        nsw       crews       dies  \n",
       "3      murder         wa       blaze     killed  \n",
       "4       faces    podcast      sydney      fatal  \n",
       "5     charges      rural      threat      woman  \n",
       "6        told        qld      school       road  \n",
       "7        case         sa        home     driver  \n",
       "8        high        vic    destroys      plane  \n",
       "9       hears        tas  suspicious       dead  \n",
       "10      trial    october   residents    injured  \n",
       "11        sex     august     factory   hospital  \n",
       "12       drug   november     warning    highway  \n",
       "13      woman  september      season      truck  \n",
       "14     appeal   december         ban        bus  \n",
       "15    alleged       july     service   accident  \n",
       "16      child         nt   melbourne        hit  \n",
       "17      death       june       south      train  \n",
       "18     fronts      april       north       near  \n",
       "19    assault      march     damages      kills  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic # 01</th>\n      <th>Topic # 02</th>\n      <th>Topic # 03</th>\n      <th>Topic # 04</th>\n      <th>Topic # 05</th>\n      <th>Topic # 06</th>\n      <th>Topic # 07</th>\n      <th>Topic # 08</th>\n      <th>Topic # 09</th>\n      <th>Topic # 10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>interview</td>\n      <td>man</td>\n      <td>police</td>\n      <td>new</td>\n      <td>says</td>\n      <td>abc</td>\n      <td>court</td>\n      <td>country</td>\n      <td>fire</td>\n      <td>crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>michael</td>\n      <td>charged</td>\n      <td>probe</td>\n      <td>zealand</td>\n      <td>council</td>\n      <td>news</td>\n      <td>accused</td>\n      <td>hour</td>\n      <td>house</td>\n      <td>car</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>extended</td>\n      <td>murder</td>\n      <td>investigate</td>\n      <td>laws</td>\n      <td>govt</td>\n      <td>rural</td>\n      <td>face</td>\n      <td>nsw</td>\n      <td>crews</td>\n      <td>dies</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>david</td>\n      <td>jailed</td>\n      <td>missing</td>\n      <td>year</td>\n      <td>plan</td>\n      <td>business</td>\n      <td>murder</td>\n      <td>wa</td>\n      <td>blaze</td>\n      <td>killed</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>john</td>\n      <td>missing</td>\n      <td>search</td>\n      <td>hospital</td>\n      <td>water</td>\n      <td>weather</td>\n      <td>faces</td>\n      <td>podcast</td>\n      <td>sydney</td>\n      <td>fatal</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>james</td>\n      <td>stabbing</td>\n      <td>death</td>\n      <td>home</td>\n      <td>nt</td>\n      <td>sport</td>\n      <td>charges</td>\n      <td>rural</td>\n      <td>threat</td>\n      <td>woman</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>nrl</td>\n      <td>guilty</td>\n      <td>hunt</td>\n      <td>york</td>\n      <td>australia</td>\n      <td>market</td>\n      <td>told</td>\n      <td>qld</td>\n      <td>school</td>\n      <td>road</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ben</td>\n      <td>found</td>\n      <td>officer</td>\n      <td>centre</td>\n      <td>health</td>\n      <td>national</td>\n      <td>case</td>\n      <td>sa</td>\n      <td>home</td>\n      <td>driver</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>matt</td>\n      <td>arrested</td>\n      <td>shooting</td>\n      <td>deal</td>\n      <td>urged</td>\n      <td>analysis</td>\n      <td>high</td>\n      <td>vic</td>\n      <td>destroys</td>\n      <td>plane</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>smith</td>\n      <td>death</td>\n      <td>seek</td>\n      <td>years</td>\n      <td>report</td>\n      <td>entertainment</td>\n      <td>hears</td>\n      <td>tas</td>\n      <td>suspicious</td>\n      <td>dead</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>andrew</td>\n      <td>sydney</td>\n      <td>arrest</td>\n      <td>gets</td>\n      <td>funding</td>\n      <td>talks</td>\n      <td>trial</td>\n      <td>october</td>\n      <td>residents</td>\n      <td>injured</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>luke</td>\n      <td>assault</td>\n      <td>find</td>\n      <td>opens</td>\n      <td>boost</td>\n      <td>speaks</td>\n      <td>sex</td>\n      <td>august</td>\n      <td>factory</td>\n      <td>hospital</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>scott</td>\n      <td>attack</td>\n      <td>drug</td>\n      <td>president</td>\n      <td>calls</td>\n      <td>reporter</td>\n      <td>drug</td>\n      <td>november</td>\n      <td>warning</td>\n      <td>highway</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>nathan</td>\n      <td>dies</td>\n      <td>assault</td>\n      <td>chief</td>\n      <td>government</td>\n      <td>share</td>\n      <td>woman</td>\n      <td>september</td>\n      <td>season</td>\n      <td>truck</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ivan</td>\n      <td>child</td>\n      <td>station</td>\n      <td>set</td>\n      <td>qld</td>\n      <td>exchange</td>\n      <td>appeal</td>\n      <td>december</td>\n      <td>ban</td>\n      <td>bus</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>mark</td>\n      <td>pleads</td>\n      <td>body</td>\n      <td>named</td>\n      <td>minister</td>\n      <td>quiz</td>\n      <td>alleged</td>\n      <td>july</td>\n      <td>service</td>\n      <td>accident</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>chris</td>\n      <td>shooting</td>\n      <td>help</td>\n      <td>life</td>\n      <td>m</td>\n      <td>australian</td>\n      <td>child</td>\n      <td>nt</td>\n      <td>melbourne</td>\n      <td>hit</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>tim</td>\n      <td>sex</td>\n      <td>fatal</td>\n      <td>south</td>\n      <td>mp</td>\n      <td>qld</td>\n      <td>death</td>\n      <td>june</td>\n      <td>south</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>peter</td>\n      <td>search</td>\n      <td>chase</td>\n      <td>ceo</td>\n      <td>labor</td>\n      <td>breakfast</td>\n      <td>fronts</td>\n      <td>april</td>\n      <td>north</td>\n      <td>near</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>steve</td>\n      <td>shot</td>\n      <td>car</td>\n      <td>rules</td>\n      <td>budget</td>\n      <td>friday</td>\n      <td>assault</td>\n      <td>march</td>\n      <td>damages</td>\n      <td>kills</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "get_topics(model, 20)"
   ]
  },
  {
   "source": [
    "We see some coeherence in the topics that the NMF model has identified. In particular:\n",
    "\n",
    "- Topic 01 has \"interview\" together with a bunch of first names, so it looks like it's identifying headlines whose topic is interviews of people\n",
    "- Topic 02 seems to identify headlines whose subject is violent crimes\n",
    "- Topic 03 is similar to topic 02, but seems to focus more on headlines having to do with police investigations\n",
    "- Topic 04 is not fully clear\n",
    "- Topic 05 identifies headlines that have to do with government or politics, such as funding, budgets, and so forth\n",
    "- Topic 06 is not clear\n",
    "- Topic 07 has also to do with crime headlines; however it focuses more on the legal aspect of these, as witnessed by the words \"court\", \"accused\", \"face\" (probably used in sentences like \"faces X years in prison\"), and so forth\n",
    "- Topic 08 seems to be isolating headlines that specify temporal details (see all the month names)\n",
    "- Topic 09 is natural disasters\n",
    "- Topic 10 is disasters of a human nature, in particular transport accidents (car crashes, et cetera)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can now use the above model to identify, for each document, which topics apply to that document."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = model.transform(X=tf_idf_matrix)\n",
    "document_topics = pd.DataFrame(document_topics)\n",
    "main_topic = document_topics.idxmax(axis=1)\n",
    "topics = [\"Topic \" + str(x) for x in range(1, 11)]\n",
    "main_topic = main_topic.apply(lambda x: topics[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['main_topic'] = main_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     publish_date                                      headline_text  \\\n",
       "35       20030219   death toll continues to climb in s korean subway   \n",
       "62       20030219            greens offer police station alternative   \n",
       "68       20030219                harrington raring to go after break   \n",
       "72       20030219                inquest finds mans death accidental   \n",
       "73       20030219  investigations underway into death toll of korean   \n",
       "98       20030219      more than 40 pc of young men drink alcohol at   \n",
       "114      20030219        nth koreans seek asylum at japanese embassy   \n",
       "132      20030219              police cracking down on driver safety   \n",
       "133      20030219         police defend aboriginal tent embassy raid   \n",
       "161      20030219    search continues for victims in s korean subway   \n",
       "171      20030219                 still no sign of missing fisherman   \n",
       "178      20030219      tasmanian scientists to search for east coast   \n",
       "213      20030220                beware the standard alcoholic drink   \n",
       "225      20030220              celts underdogs for uefa clash oneill   \n",
       "293      20030220           immigration raid in melbourne draws flak   \n",
       "337      20030220       more than 60 detained after immigration raid   \n",
       "343      20030220                    nats seek probe bushfires probe   \n",
       "364      20030220      police continue probe into human remains find   \n",
       "365      20030220                   police find second cannabis crop   \n",
       "366      20030220       police hope to gain an edge on weapons users   \n",
       "\n",
       "    main_topic  \n",
       "35     Topic 3  \n",
       "62     Topic 3  \n",
       "68     Topic 3  \n",
       "72     Topic 3  \n",
       "73     Topic 3  \n",
       "98     Topic 3  \n",
       "114    Topic 3  \n",
       "132    Topic 3  \n",
       "133    Topic 3  \n",
       "161    Topic 3  \n",
       "171    Topic 3  \n",
       "178    Topic 3  \n",
       "213    Topic 3  \n",
       "225    Topic 3  \n",
       "293    Topic 3  \n",
       "337    Topic 3  \n",
       "343    Topic 3  \n",
       "364    Topic 3  \n",
       "365    Topic 3  \n",
       "366    Topic 3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>publish_date</th>\n      <th>headline_text</th>\n      <th>main_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>35</th>\n      <td>20030219</td>\n      <td>death toll continues to climb in s korean subway</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>20030219</td>\n      <td>greens offer police station alternative</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>20030219</td>\n      <td>harrington raring to go after break</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>20030219</td>\n      <td>inquest finds mans death accidental</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>20030219</td>\n      <td>investigations underway into death toll of korean</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>20030219</td>\n      <td>more than 40 pc of young men drink alcohol at</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>20030219</td>\n      <td>nth koreans seek asylum at japanese embassy</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>20030219</td>\n      <td>police cracking down on driver safety</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>20030219</td>\n      <td>police defend aboriginal tent embassy raid</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>20030219</td>\n      <td>search continues for victims in s korean subway</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>20030219</td>\n      <td>still no sign of missing fisherman</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>20030219</td>\n      <td>tasmanian scientists to search for east coast</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>213</th>\n      <td>20030220</td>\n      <td>beware the standard alcoholic drink</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>20030220</td>\n      <td>celts underdogs for uefa clash oneill</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>293</th>\n      <td>20030220</td>\n      <td>immigration raid in melbourne draws flak</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>20030220</td>\n      <td>more than 60 detained after immigration raid</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>20030220</td>\n      <td>nats seek probe bushfires probe</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>20030220</td>\n      <td>police continue probe into human remains find</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>365</th>\n      <td>20030220</td>\n      <td>police find second cannabis crop</td>\n      <td>Topic 3</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>20030220</td>\n      <td>police hope to gain an edge on weapons users</td>\n      <td>Topic 3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "news_df.loc[lambda x: x.main_topic == \"Topic 3\"].iloc[:20]"
   ]
  }
 ]
}